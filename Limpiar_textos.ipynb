{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Ejercicio1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adanfernandez/Data-Mining/blob/main/Limpiar_textos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxXAn3F4IDip"
      },
      "source": [
        "# Primer ejercicio\n",
        "## Preparación del entorno\n",
        "Preparamos el entorno instalando las dependencias necesarias.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzBdhjgoISkT"
      },
      "source": [
        "!pip install spacy==3.0.5\n",
        "!pip install ndjson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfQ7TbhOIdeJ"
      },
      "source": [
        "Instalamos el modelo que se usará en spaCy. Se instalará el modelo grande."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-lR50EXIfjf"
      },
      "source": [
        "!python -m spacy download es_core_news_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO-s-YeaIlZm"
      },
      "source": [
        "## Descarga de los datos de la máquina virtual\n",
        "Importamos el archivo de datos desde el enlace Google Drive otorgado por el profesor de la asignatura. Para el primer ejercicio, que es el primero en el que nos encontramos, descargaremos el archivo con los datos procedentes de la API de twitter. El objetivo será limpiarlos.\n",
        "La descarga de los tweets tardará más de dos minutos en producirse, realizándose en un directorio específico para el ejercicio 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ww758RVIrwK"
      },
      "source": [
        "!mkdir ejercicio1\n",
        "%cd ejercicio1\n",
        "\n",
        "!gdown --id 1yBH-MYb3FapTWQ9ps3dkSjhaGOZKfNMK\n",
        "!unzip tweets.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILwMNTi4IvFj"
      },
      "source": [
        "Eliminamos el fichero .zip para no agotar el espacio disponible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHBgia22I2dk"
      },
      "source": [
        "!rm tweets.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KMTTJ3LI5D3"
      },
      "source": [
        "Se cargan los datos en memoria para posteriormente procesarlos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW9h8rcmI7i9"
      },
      "source": [
        "import os\n",
        "def cargar_tweets(directorio):\n",
        "    lista = {}\n",
        "    with os.scandir(directorio) as subdirectorios:\n",
        "        for subdirectorio in subdirectorios:\n",
        "            lista[directorio+subdirectorio.name] = os.listdir(directorio+subdirectorio.name)\n",
        "    return lista\n",
        "\n",
        "elementos_memoria = cargar_tweets(\"/content/ejercicio1/\")\n",
        "print(elementos_memoria)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU0pd_M9KWHm"
      },
      "source": [
        "## Limpieza de textos\n",
        "Se usará la librería ndjson https://pypi.org/project/ndjson/ para leer los archivos, además de la biblioteca io.\n",
        "Por otro lado, se usará el paquete spacy para obtener tanto los tokens como los sintagmas nominales de los tweets escritos en castellano."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK1xP-fRKW3Q"
      },
      "source": [
        "import json\n",
        "import io\n",
        "import ndjson\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "def limpiar_tweets(lista):\n",
        "    json_data = []\n",
        "    nlp = spacy.load(\"es_core_news_lg\")\n",
        "    for subdirectorio in lista:\n",
        "        for fichero in lista[subdirectorio]:\n",
        "          ruta = subdirectorio + \"/\" + fichero\n",
        "          get_tweets(ruta, nlp)\n",
        "        break\n",
        "\n",
        "def get_tweets(ruta, nlp):\n",
        "  english_tweets = []\n",
        "  tweets = []\n",
        "  with open(ruta) as f:\n",
        "    datos = ndjson.load(f)\n",
        "    for entrada in datos:\n",
        "      if 'lang' in entrada and entrada['lang']==\"es\" or 'lang' not in entrada and entrada['user']['lang'] == 'es':\n",
        "        hashtags = []\n",
        "        for hashtag in entrada['entities']['hashtags']:\n",
        "          hashtags.append(hashtag['text'])\n",
        "        tweet_append = {\n",
        "            \"created_at\": entrada['created_at'],\n",
        "            \"id_str\": entrada['id_str'],\n",
        "            \"text\": entrada['text'],\n",
        "            \"user\": {\n",
        "                \"screen_name\": entrada['user']['screen_name'],\n",
        "                \"description\": entrada['user']['description']\n",
        "            },\n",
        "            \"hashtags\": hashtags,\n",
        "            \"urls\": entrada['entities']['urls']\n",
        "        }\n",
        "        doc = nlp(entrada['text'])\n",
        "        noum_chunks = []\n",
        "        for chunk in doc.noun_chunks:\n",
        "          noum_chunks.append(chunk.root.text)\n",
        "        tweet_append['noum_chunks'] = noum_chunks\n",
        "        tokens = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_stop]\n",
        "        tweet_append['tokens'] = tokens\n",
        "        tweets.append(tweet_append)\n",
        "    return tweets\n",
        "\n",
        "tweets_limpios = limpiar_tweets(elementos_memoria)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}