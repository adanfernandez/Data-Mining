{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextRank_SimHash.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adanfernandez/Data-Mining/blob/main/TextRank_SimHash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bhjNxTgDsbe"
      },
      "source": [
        "# Text Rank & Simhash\n",
        "## Preparación del entorno"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI1KSPsEDkZA"
      },
      "source": [
        "!pip install spacy==3.0.5\n",
        "!pip install simhash\n",
        "!pip install langdetect\n",
        "!pip install shifterator\n",
        "!pip install ndjson\n",
        "!pip install pytextrank\n",
        "!python -m spacy download es_core_news_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMBP3QoOENjh"
      },
      "source": [
        "Descargamos los datos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y03KbiHHEQh6"
      },
      "source": [
        "%cd /content\n",
        "!mkdir limpio\n",
        "!mkdir ejercicio2\n",
        "%cd limpio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-dVeizRIIsk"
      },
      "source": [
        "!gdown --id 1brYy8Tiooo2Uw--FUmPb8lpkxJGM4cnR\n",
        "!unzip tweets-limpios.ndjson.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uhNckyqIeIa"
      },
      "source": [
        "## Cuerpo del ejercicio\n",
        "Obtenemos los tweets por días. Esta operación tardará en torno a los 3 minutos y medio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-r8BVY_Idho"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import io\n",
        "import os\n",
        "import ndjson\n",
        "import math\n",
        "\n",
        "\n",
        "SPANISH_STOPWORDS = {'a','al','algo','algunas','algunos','ante','antes','como',\n",
        "                     'con','contra','cual','cuando','de','del','desde','donde',\n",
        "                     'durante','e','el','él','ella','ellas','ellos','en','entre',\n",
        "                     'era','erais','éramos','eran','eras','eres','es','esa',\n",
        "                     'esas','ese','eso','esos','esta','está','estaba','estabais',\n",
        "                     'estábamos','estaban','estabas','estad','estada','estadas',\n",
        "                     'estado','estados','estáis','estamos','están','estando',\n",
        "                     'estar','estará','estarán','estarás','estaré','estaréis',\n",
        "                     'estaremos','estaría','estaríais','estaríamos','estarían',\n",
        "                     'estarías','estas','estás','este','esté','estéis','estemos',\n",
        "                     'estén','estés','esto','estos','estoy','estuve','estuviera',\n",
        "                     'estuvierais','estuviéramos','estuvieran','estuvieras',\n",
        "                     'estuvieron','estuviese','estuvieseis','estuviésemos',\n",
        "                     'estuviesen','estuvieses','estuvimos','estuviste',\n",
        "                     'estuvisteis','estuvo','fue','fuera','fuerais','fuéramos',\n",
        "                     'fueran','fueras','fueron','fuese','fueseis','fuésemos',\n",
        "                     'fuesen','fueses','fui','fuimos','fuiste','fuisteis','ha',\n",
        "                     'habéis','había','habíais','habíamos','habían','habías',\n",
        "                     'habida','habidas','habido','habidos','habiendo','habrá',\n",
        "                     'habrán','habrás','habré','habréis','habremos','habría',\n",
        "                     'habríais','habríamos','habrían','habrías','han','has',\n",
        "                     'hasta','hay','haya','hayáis','hayamos','hayan','hayas',\n",
        "                     'he','hemos','hube','hubiera','hubierais','hubiéramos',\n",
        "                     'hubieran','hubieras','hubieron','hubiese','hubieseis',\n",
        "                     'hubiésemos','hubiesen','hubieses','hubimos','hubiste',\n",
        "                     'hubisteis','hubo','la','las','le','les','lo','los','más',\n",
        "                     'me','mi','mí','mía','mías','mío','míos','mis','mucho',\n",
        "                     'muchos','muy','nada','ni','no','nos','nosotras','nosotros',\n",
        "                     'nuestra','nuestras','nuestro','nuestros','o','os','otra',\n",
        "                     'otras','otro','otros','para','pero','poco','por','porque',\n",
        "                     'que','qué','quien','quienes','se','sea','seáis','seamos',\n",
        "                     'sean','seas','será','serán','serás','seré','seréis',\n",
        "                     'seremos','sería','seríais','seríamos','serían',\n",
        "                     'serías','sí','sido','siendo','sin','sobre','sois','somos',\n",
        "                     'son','soy','su','sus','suya','suyas','suyo','suyos','también',\n",
        "                     'tanto','te','tendrá','tendrán','tendrás','tendré','tendréis',\n",
        "                     'tendremos','tendría','tendríais','tendríamos','tendrían',\n",
        "                     'tendrías','tened','tenéis','tenemos','tenga','tengáis',\n",
        "                     'tengamos','tengan','tengas','tengo','tenía','teníais',\n",
        "                     'teníamos','tenían','tenías','tenida','tenidas','tenido',\n",
        "                     'tenidos','teniendo','ti','tiene','tienen','tienes','todo',\n",
        "                     'todos','tu','tú','tus','tuve','tuviera','tuvierais',\n",
        "                     'tuviéramos','tuvieran','tuvieras','tuvieron','tuviese',\n",
        "                     'tuvieseis','tuviésemos','tuviesen','tuvieses','tuvimos',\n",
        "                     'tuviste','tuvisteis','tuvo','tuya','tuyas','tuyo','tuyos',\n",
        "                     'un','una','uno','unos','vosotras','vosotros','vuestra',\n",
        "                     'vuestras','vuestro','vuestros','y','ya','yo'}\n",
        "\n",
        "def quitar_tildes_n_pasar_mayusculas(texto):\n",
        "  texto = texto.lower()\n",
        "  texto = texto.replace('á', 'a')\n",
        "  texto = texto.replace('é', 'e')\n",
        "  texto = texto.replace('í', 'i')\n",
        "  texto = texto.replace('ó', 'o')\n",
        "  texto = texto.replace('ú', 'u')\n",
        "  texto = texto.replace('ñ', 'n')\n",
        "  return texto\n",
        "\n",
        "\n",
        "def tweets_dias(jsons, tweets_dias):\n",
        "  for tuit in jsons:\n",
        "    dtime = tuit['created_at']\n",
        "    timestamp = time.mktime(datetime.datetime.strptime(dtime,'%a %b %d %H:%M:%S +0000 %Y').timetuple())\n",
        "    timestamp=str(math.floor(timestamp/86400)*86400)\n",
        "    if timestamp not in tweets_dias:\n",
        "      tweets_dias[timestamp]=[]\n",
        "      diccionario = {}\n",
        "      print(timestamp)\n",
        "    tuit_append = {}\n",
        "    tuit_append['text'] = tuit['text']\n",
        "    tuit_append['tokens'] =  list(filter(lambda x: len(str(x)) > 1 and x  not in {'http','https','www','com','tinyurl','html','twitter', 'rt', 'reuters', 'bbc', 'cnn'} and x not in SPANISH_STOPWORDS, tuit['tokens']))\n",
        "    #tuit_append['tokens'] = tuit['tokens']\n",
        "    tweets_dias[timestamp].append(tuit_append)\n",
        "  return tweets_dias\n",
        "\n",
        "\n",
        "def get_tweets():\n",
        "  f = io.open(\"tweets-limpios.ndjson\", mode=\"r\", encoding=\"utf-8\")\n",
        "  tamano_leido=0\n",
        "  tamano_bloque=20*1024*1024\n",
        "  tamano = os.path.getsize(\"tweets-limpios.ndjson\")\n",
        "\n",
        "  segmentados = []\n",
        "  tweets={}\n",
        "\n",
        "  while True:\n",
        "    content = f.readlines(tamano_bloque)\n",
        "    content = \" \".join(content) # readlines devuelve una lista, lo unimos en una cadena\n",
        "    tamano_leido += tamano_bloque\n",
        "    if not content:\n",
        "      break\n",
        "    else:\n",
        "      jsons = ndjson.loads(content)\n",
        "      tweets = tweets_dias(jsons, tweets)\n",
        "  return tweets\n",
        "\n",
        "tweets = get_tweets()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ho0e5z7RAe0"
      },
      "source": [
        "Se define una función para obtener n elementos aleatorios de una lista. En este caso elegimos 10 mil."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBEKgXA0PXv2"
      },
      "source": [
        "import random\n",
        "\n",
        "def get_random_from_list(n=10000):\n",
        "  random_list = {}\n",
        "  aux = n\n",
        "\n",
        "  for key in tweets.keys():\n",
        "    n = aux\n",
        "    if n > len(tweets[key]):\n",
        "      n=len(tweets[key])\n",
        "    random_list[key] = random.sample(tweets[key], n)\n",
        "  return random_list\n",
        "\n",
        "random_tweets = get_random_from_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RTh_dX0EeLO"
      },
      "source": [
        "Obtenemos todos los tokens por día. Es decir, un token por cada clave (timestamp)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSytQ1wOqdyH"
      },
      "source": [
        "tokens_dia = {}\n",
        "for key in random_tweets.keys():\n",
        "  tokens_dia[key] = []\n",
        "  for tweet in random_tweets[key]:\n",
        "    for token in tweet['tokens']:\n",
        "        tokens_dia[key].append(token)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oDyV0W9sMHd"
      },
      "source": [
        "Aplicamos textRank a cada día. Es decir, se aplicará a la lista de elementos tokenizados que contiene cada día."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDaZH-eZsOEF"
      },
      "source": [
        "import pytextrank\n",
        "import spacy\n",
        "\n",
        "def aplicar_textrank(texto, nlp):\n",
        "  doc = nlp(texto)\n",
        "  lemas = doc._.textrank.ranks\n",
        "  lemas = sorted(lemas.items(), key=lambda x: x[1],reverse=True)\n",
        "  resultados = {}\n",
        "  resultados[\"lemas\"] = lemas\n",
        "  return resultados\n",
        "\n",
        "max_caracteres = 1000000\n",
        "\n",
        "def aplicar_textrank_tweets_tokenizados_dia():\n",
        "  resultado = {}\n",
        "  nlp = spacy.load(\"es_core_news_lg\")\n",
        "  nlp.add_pipe(\"textrank\")\n",
        "  with tqdm(total=len(tokens_dia.keys())) as barra:\n",
        "    for key in tokens_dia.keys():\n",
        "      barra.update(1)\n",
        "      text =  \" \".join(tokens_dia[key])[0:max_caracteres]\n",
        "      resultado [key] = aplicar_textrank(text, nlp)\n",
        "    return resultado\n",
        "\n",
        "ranking = aplicar_textrank_tweets_tokenizados_dia()\n",
        "print(\"El ranking ha terminado.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRIwpT6WDfbn"
      },
      "source": [
        "Visualizamos el ranking de text rank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwZ38gDhDdf-"
      },
      "source": [
        "import pprint\n",
        "\n",
        "\n",
        "for key in ranking:\n",
        "  ranking_dia = ranking[key]\n",
        "  for lema in ranking_dia[\"lemas\"][0:30]:\n",
        "    termino = lema[0]\n",
        "    puntuacion = lema[1]\n",
        "    print(termino.lemma, \"\\t\", puntuacion)\n",
        "    #if termino.pos=='NOUN':\n",
        "     # print(termino.lemma, \"\\t\", puntuacion)\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT11F4wzGQRm"
      },
      "source": [
        "Tras esto, calcularemos el text rank de cada tweet, pudiendo sacar los mil tweets con mayor puntuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4847DLQU3WHa"
      },
      "source": [
        "puntuaciones = {}\n",
        "\n",
        "for day in tweets:\n",
        "  puntuaciones[day] = []\n",
        "  for tweet in tweets[day]:\n",
        "    puntuacion = 0.0\n",
        "    ranking_dia = ranking[day]\n",
        "    for lema in ranking_dia[\"lemas\"][0:30]:\n",
        "      if lema[0].lemma in tweet['tokens']:\n",
        "        puntuacion += lema[1]\n",
        "    tweet['puntuacion'] = 0.0\n",
        "    if len(tweet['tokens']) != 0:\n",
        "      tweet['puntuacion'] = puntuacion/(len(tweet['tokens']))\n",
        "    puntuaciones[day].append(tweet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b4P_dWqAhpp"
      },
      "source": [
        "Cogemos los mil primeros tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmmXG_NiPaBn"
      },
      "source": [
        "orden = {}\n",
        "for day in puntuaciones.keys():\n",
        "  orden[day] = sorted(puntuaciones[day], key=lambda x: x['puntuacion'], reverse=True)[0:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqZPfJsADTx7"
      },
      "source": [
        "Eliminamos duplicados aplicando simhash a los mil más relevantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZbYgwz5DWkP"
      },
      "source": [
        "from simhash import Simhash, SimhashIndex\n",
        "\n",
        "def duplicados(tweets):\n",
        "  firmas = []\n",
        "  tweets_limpios  = []\n",
        "  for complete_tweet in tweets:\n",
        "    tweet = complete_tweet['text']\n",
        "    firma = Simhash(tweet)\n",
        "    firmas.append((tweet, firma))\n",
        "  \n",
        "  indice = SimhashIndex(firmas, k=10)\n",
        "  indice.log.setLevel(0)\n",
        "  for tweet in tweets:\n",
        "    firma = Simhash(tweet['text'])\n",
        "    duplicados = indice.get_near_dups(firma)\n",
        "    for dup in duplicados:\n",
        "      if dup not in tweets_limpios:\n",
        "        tweets_limpios.append(dup)\n",
        "      break\n",
        "  return tweets_limpios\n",
        "  \n",
        "tweets_simhash={}\n",
        "with tqdm(total=len(orden.keys())) as barra:\n",
        "  for key in orden.keys():\n",
        "    barra.update(1)\n",
        "    tweets_simhash[key] = duplicados(orden[key])\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2RGysrtQSHh"
      },
      "source": [
        "for ors in tweets_simhash.keys():\n",
        "  print(len(tweets_simhash[ors]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCXDNtJKB5mt"
      },
      "source": [
        "Visualizamos  los tweets por día..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnnkmkUDB5LK"
      },
      "source": [
        "for day in orden:\n",
        "  for tweet in tweets_simhash[day]:\n",
        "    print(tweet)\n",
        "print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n OTRO DÍA...\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}